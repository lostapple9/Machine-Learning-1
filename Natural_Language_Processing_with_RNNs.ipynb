{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing with RNNs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUgzCUtZ9VEPQ6a7fUszEF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lostapple9/Machine-Learning-1/blob/main/Natural_Language_Processing_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=tPYj3fFJGjk&list=LL&index=13&t=16948s&ab_channel=freeCodeCamp.org\n",
        "\n",
        "https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true&scrollTo=01BJLcGb4ZqK\n",
        "\n",
        "recurrent neural network (rnn)"
      ],
      "metadata": {
        "id": "K9tEz1rbS7uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpU9sLw83pAD",
        "outputId": "46dab440-a59d-42f2-8f3f-3b91d00a537d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_bag = bag_of_words(positive_review)\n",
        "neg_bag = bag_of_words(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_bag)\n",
        "print(\"Negative:\", neg_bag)"
      ],
      "metadata": {
        "id": "PItrK646qwG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3725e16c-8a5c-403c-c241-b36aab0c7c52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
            "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}  \n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \") \n",
        "  encoding = []  \n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]  \n",
        "      encoding.append(code) \n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "  \n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wjZwXGJ430V",
        "outputId": "5ddd38b2-aa76-4fcf-fa51-d131f782996c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = one_hot_encoding(positive_review)\n",
        "neg_encode = one_hot_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "metadata": {
        "id": "MxDuNzxqqxob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b1a94c-f4ad-45a6-e1da-d7db94719100"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
            "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QvaEKeBoqxuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cdxcoDUNqxyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "1WcT-TREqx1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffead85d-9242-4102-c8b3-a05ce8454025"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at one review\n",
        "train_data[1]\n",
        "len(train_data[1])"
      ],
      "metadata": {
        "id": "OiHuebxsqx4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc4b28e-94d8-4f56-b4f5-d710fbddd6db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "len(train_data[1])\n",
        "train_data"
      ],
      "metadata": {
        "id": "XGAyDb7Hqx7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c94b473-d432-4f68-e842-5723d6fd0238"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,    19,   178,    32],\n",
              "       [    0,     0,     0, ...,    16,   145,    95],\n",
              "       [    0,     0,     0, ...,     7,   129,   113],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     4,  3586, 22459],\n",
              "       [    0,     0,     0, ...,    12,     9,    23],\n",
              "       [    0,     0,     0, ...,   204,   131,     9]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "HRGjOJlOqx-T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "F4TLdxeFqyBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da922e34-7a9c-491e-de8a-ef9b40d21153"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.99)"
      ],
      "metadata": {
        "id": "YWWjW0BwqyEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81be2682-2fcd-4c1a-dec7-61a4ee4c684e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 19s 2s/step - loss: 0.6727 - acc: 0.6200 - val_loss: 0.6249 - val_acc: 0.7305\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.5501 - acc: 0.7880 - val_loss: 0.5753 - val_acc: 0.7069\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.4123 - acc: 0.8840 - val_loss: 0.5929 - val_acc: 0.6718\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.3552 - acc: 0.8920 - val_loss: 0.5524 - val_acc: 0.7335\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.2443 - acc: 0.9880 - val_loss: 0.5144 - val_acc: 0.7505\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.2303 - acc: 0.9720 - val_loss: 0.5076 - val_acc: 0.7514\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.1323 - acc: 0.9960 - val_loss: 0.6355 - val_acc: 0.6890\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.1082 - acc: 0.9840 - val_loss: 0.5936 - val_acc: 0.7316\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 16s 2s/step - loss: 0.0900 - acc: 1.0000 - val_loss: 0.5981 - val_acc: 0.7161\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 17s 2s/step - loss: 0.0629 - acc: 1.0000 - val_loss: 0.8016 - val_acc: 0.6742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "iReCTW8xqyG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553782d8-5de2-4279-8ccd-33d9f838f6b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 18s 22ms/step - loss: 0.8017 - acc: 0.6736\n",
            "[0.8016678094863892, 0.6735600233078003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"Death is the solution to all problems. No man - no problem\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)\n"
      ],
      "metadata": {
        "id": "cIWq1B3KqyJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2feaa23e-7984-4e58-b4a2-57a286acbd72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0  338    6    1 4785    5   29  709   54  129   54  436]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while were at it lets make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "    PAD = 0\n",
        "    text = \"\"\n",
        "    for num in integers:\n",
        "      if num != PAD:\n",
        "        text += reverse_word_index[num] + \" \"\n",
        "\n",
        "    return text[:-1]\n",
        "  \n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "id": "rbzX9j87rUS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47239cfb-a4bc-4cef-c20e-04d6033f2156"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "death is the solution to all problems no man no problem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)\n"
      ],
      "metadata": {
        "id": "wwXHae9drUXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652d58d0-b76f-4ca9-d13d-b43acaf24a5e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.92673254]\n",
            "[0.9035412]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vr6H5eddrUab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Play Generator\n",
        "\n",
        "Now time for one of the coolest examples we've seen so far. We are going to use a RNN to generate a play. We will simply show the RNN an example of something we want it to recreate and it will learn how to write a version of it on its own. We'll do this using a character predictive model that will take as input a variable length sequence and predict the next character. We can use the model many times in a row with the output from the last predicition as the input for the next call to generate a sequence.\n",
        "\n",
        "\n",
        "*This guide is based on the following: https://www.tensorflow.org/tutorials/text/text_generation*"
      ],
      "metadata": {
        "id": "ky_BIswnCR3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p3Wa5FXGrUdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "KPM-AsB2rUgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156d2cab-b804-4532-8b6c-c7f876793a61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "h3RFujxXrUio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff83a6c-14f5-4842-a81d-5b3190d3d6e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "dCGSO5rGBjZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guhemVXeBjfE",
        "outputId": "8bb65782-7838-426c-9488-477d8fc7ecce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3a7fyOEBjm0",
        "outputId": "e5a9bf63-76b8-4a22-a32e-9cbf2edf7d24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "Mi5RrrF5Bjp7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOvf-EufBjxP",
        "outputId": "cb53e70d-9a38-44a9-fd61-24c04e9346d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7ASfxe8Bj0P",
        "outputId": "66fb774a-ba09-4f81-dbce-c29aec1df22e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "u40aBxQLBplw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "TRu4IsQPBppN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "TbWxOlCcBps3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyctR9EUBpvx",
        "outputId": "e61036dc-5098-4bbb-dd4c-cedf7bffdc49"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "11fUglpdBpys"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxDEUn6QBu1G",
        "outputId": "342ee2bf-a232-4d36-9546-4d485b27c07b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EVPi3ZtBu3v",
        "outputId": "de4091c2-97a1-41b7-bb9e-61b178777a33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0djzCg5Bu6c",
        "outputId": "4d29a0c4-cad4-4e68-a202-4067913995ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-2.7568711e-03 -3.3817876e-03 -2.7134812e-03 ...  4.9917144e-03\n",
            "    2.6450437e-03  8.7628560e-04]\n",
            "  [ 1.6775844e-03 -7.4807773e-03 -4.7098943e-03 ...  5.5401464e-04\n",
            "   -2.4678628e-03  2.2021420e-03]\n",
            "  [ 2.9961660e-03 -7.2992025e-03 -4.3764999e-03 ... -2.1611666e-03\n",
            "    1.6025380e-04 -5.8533368e-04]\n",
            "  ...\n",
            "  [ 1.0664448e-02 -1.3610829e-02 -4.8654750e-03 ... -2.3068420e-03\n",
            "    8.8103367e-03  2.2477044e-03]\n",
            "  [ 5.2844402e-03 -1.4002364e-02 -6.9910279e-03 ...  3.5343955e-03\n",
            "    9.7321440e-03  3.0650478e-03]\n",
            "  [ 7.8202914e-03 -1.5843622e-02 -8.2678767e-03 ... -2.0088159e-04\n",
            "    3.3097942e-03  4.2608688e-03]]\n",
            "\n",
            " [[ 1.6640723e-03 -2.5949476e-03 -1.4087398e-03 ... -4.4505945e-03\n",
            "    2.1708068e-03  3.4734893e-03]\n",
            "  [-7.1785680e-04 -4.5250336e-04 -8.4495485e-05 ... -1.1942316e-02\n",
            "    4.6428954e-03  9.8715071e-03]\n",
            "  [ 6.1919412e-04 -1.2146594e-03 -1.6433449e-03 ... -1.1676334e-02\n",
            "    6.8951081e-03  5.5377702e-03]\n",
            "  ...\n",
            "  [ 2.4813279e-03 -1.0278642e-02  5.0750934e-03 ... -1.4443089e-02\n",
            "    6.2158694e-03  1.2086618e-02]\n",
            "  [ 4.4411332e-03 -9.1421977e-03  2.2259450e-03 ... -1.2363350e-02\n",
            "    8.1190774e-03  7.2325217e-03]\n",
            "  [ 7.2287046e-04 -1.0417001e-02 -1.9341023e-03 ... -3.5271146e-03\n",
            "    9.8784547e-03  6.4984947e-03]]\n",
            "\n",
            " [[ 1.4949245e-03 -9.4802055e-04 -1.3847931e-03 ... -2.8961920e-03\n",
            "    2.2865813e-03 -1.7894373e-03]\n",
            "  [ 3.9414736e-03  1.7942863e-03  1.8010392e-04 ... -7.5209001e-03\n",
            "    3.9828746e-03 -4.9831351e-04]\n",
            "  [-1.1623671e-03 -1.2143919e-03 -2.5269735e-04 ... -5.6248643e-03\n",
            "    8.1152527e-04  5.9691193e-03]\n",
            "  ...\n",
            "  [-3.9484719e-04 -9.3875476e-04  1.5696718e-03 ... -1.8636553e-02\n",
            "    6.5863784e-03  1.0831075e-02]\n",
            "  [-2.8423260e-03 -4.5623486e-03 -2.2608298e-03 ... -8.3841980e-03\n",
            "    8.9063002e-03  8.4618069e-03]\n",
            "  [-3.0860670e-03 -2.5585762e-03  1.2485832e-03 ...  1.8161600e-03\n",
            "    1.0731587e-02  2.4694246e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 8.1919441e-03 -3.0109475e-03 -9.4115781e-04 ...  2.8362637e-03\n",
            "    5.5583809e-03 -7.1483562e-03]\n",
            "  [ 8.0250334e-03 -4.2395946e-03 -2.2718140e-03 ... -1.6474853e-03\n",
            "    6.1604814e-03 -1.0408487e-03]\n",
            "  [ 5.5681234e-03 -4.3433490e-03 -4.1438174e-03 ... -2.7866507e-04\n",
            "   -4.6616586e-04 -4.5137671e-03]\n",
            "  ...\n",
            "  [ 9.5904358e-03 -6.7547928e-03  9.6692704e-05 ... -8.3522368e-03\n",
            "    6.1223046e-03 -1.0301251e-02]\n",
            "  [ 9.3348082e-03 -8.0669997e-03 -2.3516729e-03 ... -1.0202954e-02\n",
            "    8.3609642e-03 -4.4311234e-03]\n",
            "  [ 6.6542672e-03 -3.2100859e-03 -4.6069769e-04 ... -1.0975910e-02\n",
            "    9.4598560e-03 -3.4529292e-03]]\n",
            "\n",
            " [[ 5.4770322e-03 -5.8970205e-03  2.8840429e-03 ... -7.8020864e-03\n",
            "    1.7087432e-03  1.7727962e-03]\n",
            "  [ 6.6918666e-03 -6.2506441e-03  2.8449831e-05 ... -8.3234841e-03\n",
            "    4.5426101e-03 -5.5709336e-04]\n",
            "  [-8.4097602e-04 -3.2381697e-03 -4.0049255e-03 ... -4.5992313e-03\n",
            "    8.1787454e-03 -3.8471129e-03]\n",
            "  ...\n",
            "  [ 1.5925644e-03 -1.4812737e-03 -9.4246296e-03 ... -4.9068374e-03\n",
            "    8.8940049e-04  4.2316420e-03]\n",
            "  [ 2.7312378e-03  4.7442485e-03 -3.5067599e-03 ... -2.7002287e-04\n",
            "    5.7977666e-03  8.3616227e-03]\n",
            "  [ 2.3061230e-03  2.3339051e-03 -7.8230789e-03 ... -2.0044255e-03\n",
            "    1.6261125e-03  1.0499317e-02]]\n",
            "\n",
            " [[ 4.8745810e-03  9.7402686e-04 -6.2997517e-04 ...  6.5864236e-03\n",
            "    4.3820520e-03  3.6315841e-03]\n",
            "  [ 5.2646790e-03 -1.5504661e-03 -1.6350716e-03 ... -1.3026515e-04\n",
            "    4.0382901e-03  5.9442306e-03]\n",
            "  [-4.4016964e-03 -4.4883741e-03 -1.3395157e-03 ... -2.2181636e-03\n",
            "   -4.1192085e-03 -1.1489704e-03]\n",
            "  ...\n",
            "  [-6.9520168e-04 -1.1279174e-02 -7.9363408e-03 ... -4.3835291e-03\n",
            "    2.1788822e-03 -4.9244156e-03]\n",
            "  [ 6.9431118e-03 -1.2066327e-02 -7.0510204e-03 ... -1.0915643e-03\n",
            "    7.2286809e-03 -1.1419274e-02]\n",
            "  [ 3.3022701e-03 -7.6411716e-03 -4.2825853e-03 ... -9.2736371e-03\n",
            "    7.7735246e-03 -1.0895131e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PYVsXz6Bu87",
        "outputId": "1790344a-04df-4516-ee22-62f736534cdc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00275687 -0.00338179 -0.00271348 ...  0.00499171  0.00264504\n",
            "   0.00087629]\n",
            " [ 0.00167758 -0.00748078 -0.00470989 ...  0.00055401 -0.00246786\n",
            "   0.00220214]\n",
            " [ 0.00299617 -0.0072992  -0.0043765  ... -0.00216117  0.00016025\n",
            "  -0.00058533]\n",
            " ...\n",
            " [ 0.01066445 -0.01361083 -0.00486547 ... -0.00230684  0.00881034\n",
            "   0.0022477 ]\n",
            " [ 0.00528444 -0.01400236 -0.00699103 ...  0.0035344   0.00973214\n",
            "   0.00306505]\n",
            " [ 0.00782029 -0.01584362 -0.00826788 ... -0.00020088  0.00330979\n",
            "   0.00426087]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okZMQcxBvBY",
        "outputId": "a9b87bd3-9497-465f-f9ce-bd7dd94fb002"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-2.7568711e-03 -3.3817876e-03 -2.7134812e-03  8.7873091e-04\n",
            " -5.9595628e-04 -1.5430187e-03  2.8973111e-04  1.2007543e-03\n",
            " -1.4000967e-03 -6.7071007e-03  2.6998767e-03 -6.1708898e-03\n",
            " -5.2276528e-03 -7.6413550e-04 -1.9263417e-03 -4.0617613e-03\n",
            " -1.2429869e-03 -4.1159979e-04  4.6607084e-04  5.6571735e-04\n",
            "  3.7468208e-03 -4.4322102e-03  3.1385375e-03  2.2875988e-03\n",
            "  3.5037003e-03  5.3156540e-03  6.5193948e-04  5.1691255e-04\n",
            " -3.8822892e-03 -2.2949374e-03 -1.7031868e-03 -5.4736855e-04\n",
            " -2.5467428e-03 -2.7296604e-03  3.3365332e-03  4.4082138e-03\n",
            " -1.2335693e-03 -2.0115785e-03  7.5519970e-03  3.4615996e-03\n",
            " -2.9252889e-03 -7.4467179e-04  1.8341150e-03 -3.0513864e-03\n",
            " -3.2524367e-05  4.5730248e-03  1.9580452e-03  5.0668018e-03\n",
            " -6.3121552e-03 -2.5585038e-03  3.6525424e-03 -3.8569700e-03\n",
            " -2.4892611e-03  3.2353003e-03 -3.9882590e-03 -1.0292626e-03\n",
            " -5.0827404e-03  2.2975215e-05 -7.9700252e-04  3.1830196e-03\n",
            "  1.3720125e-03  3.2723255e-03  4.9917144e-03  2.6450437e-03\n",
            "  8.7628560e-04], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JAu0ntRvBvEh",
        "outputId": "7f32fda9-82b4-41a8-8a8a-a1f664cd43b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"$nbesb$RdS.o,xiRdbfH3! Xk roJrgMgvG3FfozXJfa:PCgEh;HDld,T?g&- IT-k.\\ndAe'3TyeP3;L3AMMq.&KfZpFF!adliJ!\""
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "v0UP3XchBvHO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "VJ9cXuxGBvKE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "jakLcSKTBvM1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY1AiPx9BvPs",
        "outputId": "c4f35577-2a20-4f32-8667-8c2ef6fdeb2f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 31s 160ms/step - loss: 2.6820\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 1.9868\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.7259\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.5795\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.4904\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.4304\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.3850\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.3482\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.3170\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2884\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2622\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2360\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.2105\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.1840\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.1574\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.1305\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.1007\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 1.0710\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.0387\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 1.0074\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.9741\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.9394\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.9061\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.8744\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.8406\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.8085\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.7783\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.7500\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.7231\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6993\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6742\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.6515\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6325\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.6135\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5962\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5799\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5660\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5518\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5397\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.5280\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5174\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.5078\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4981\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4914\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.4833\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 29s 163ms/step - loss: 0.4750\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.4673\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4635\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4580\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 29s 162ms/step - loss: 0.4523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "EdaE93LHB6ax"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "hAAA91yyB6gS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "kcadExCSB6ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "ag4rpt_1B6q0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQSU1fVuB6wD",
        "outputId": "d8e9bf8f-a975-48d6-f82c-4ffb449bd1d2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: obama\n",
            "obamands in the morning,\n",
            "And provide te my heart?\n",
            "Thou plains great king, more larks all my conveyances and one Master Froth?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Can give the leg plants on mind, and one in Berenta, sir?\n",
            "\n",
            "LARTIUS:\n",
            "\n",
            "VOLUMNIA:\n",
            "I have learn'd her he be the day,\n",
            "If Warwick be so near as is the rest.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Go, to the painte, the people\n",
            "Defy me dear-like, than sticts' to hear me; like a dear\n",
            "Enforced the prince agree,\n",
            "And let him crapping him to his unnatural sceneall'd:\n",
            "Signior Baptista, lest you be no more.\n",
            "\n",
            "All:\n",
            "Farewell.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "\n",
            "\n",
            "KING EDWARD IV:\n",
            "Black, man, obeyself; immortal pate\n",
            "To his condition from his horse.\n",
            "\n",
            "PERDITA:\n",
            "Sir, my gracious lord,--\n",
            "\n",
            "KING RICHARD III:\n",
            "I swear-blow'd; he that contempt I have remember'd\n",
            "Shall I nor at the people.\n",
            "\n",
            "AEdile:\n",
            "In joyful to his own report,\n"
          ]
        }
      ]
    }
  ]
}